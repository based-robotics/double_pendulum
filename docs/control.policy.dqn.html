<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Deep-Q Learning (DQN) &mdash; Double Pendulum 1.0.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Model Predictive Control (MPC)" href="control.mpc.html" />
    <link rel="prev" title="Soft Actor Critic (SAC)" href="control.policy.sac.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Double Pendulum
            <img src="_static/chaotic_freefall_long_exposure_shot.jpg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Table of Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="dynamics.html"> Double Pendulum Dynamics</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html"> Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/double_pendulum.html"> Software Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="software_structure.html"> Repository Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware.html"> Hardware</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="control.html"> Control Methods</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="control.trajopt.html">Trajectory Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="control.trajstab.html">Trajectory Stabilization</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="control.policy.html">Policy-based Control</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="control.policy.lqr.html">Linear Quadratic Regulator (LQR)</a></li>
<li class="toctree-l3"><a class="reference internal" href="control.policy.pfl.html">Partial Feedback Linearization (PFL)</a></li>
<li class="toctree-l3"><a class="reference internal" href="control.policy.sac.html">Soft Actor Critic (SAC)</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Deep-Q Learning (DQN)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="control.mpc.html">Model Predictive Control (MPC)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="leaderboard.html"> Leaderboards</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Double Pendulum</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="modules.html">&lt;no title&gt;</a> &raquo;</li>
          <li><a href="control.html">Control Methods</a> &raquo;</li>
          <li><a href="control.policy.html">Policy-based Control</a> &raquo;</li>
      <li>Deep-Q Learning (DQN)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/control.policy.dqn.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="deep-q-learning-dqn">
<h1>Deep-Q Learning (DQN)<a class="headerlink" href="#deep-q-learning-dqn" title="Permalink to this headline"></a></h1>
<p>The DQN controller is a reinforcement learning-based control strategy
that is particularly well-suited for scenarios with discrete action spaces.
However, discretizing the action space can be an option when its dimensionality is small.
The action space for pendubot and acrobot is only composed of one dimension.</p>
<p>In the context of the double pendulum swing-up and stabilization
tasks, the actuators controlling the double pendulum can be
adjusted to any value within the specified torque limit range.
Additionally, the measurements of position and velocity obtained
from the motors can also be represented continuously.
The problem setup makes it suitable for DQN implementation.</p>
<p>The idea of DQN is to learn an action-value function from which a greedy
policy would yield the highest possible sum of discounted rewards.
To learn such a function, this method uses the optimal Bellman operator.
This operator is a contracting mapping, meaning that the successive iterations of
this operator lead to its fixed point. The theory guarantees
that this fixed point is the optimal action-value function cor-
responding to the optimal policy i.e., the policy yielding max-
imum reward.</p>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>[1] V. Mnih, K. Kavukcuoglu, D. Silver, et al., “Human-
level control through deep reinforcement learning,” na-
ture, vol. 518, no. 7540, pp. 529–533, 2015.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="control.policy.sac.html" class="btn btn-neutral float-left" title="Soft Actor Critic (SAC)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="control.mpc.html" class="btn btn-neutral float-right" title="Model Predictive Control (MPC)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Underactuated Lab.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>